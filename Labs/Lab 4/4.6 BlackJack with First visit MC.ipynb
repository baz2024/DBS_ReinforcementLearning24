{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing BlackJack with First Visit MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First, let us import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Simulate the Blackjack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('Blackjack-v1', render_mode='human')\n",
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the policy function which takes the current state and check if the score is\n",
    "greater than or equal to 20, if yes we return 0 else we return 1. i.e If the score is greater\n",
    "than or equal to 20 we stand (0) else we hit (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_policy(observation):\n",
    "    score, dealer_score, usable_ace = observation\n",
    "    return 0 if score >= 20 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function called generate_episode for generating epsiodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_episode(policy, env):\n",
    "    \n",
    "    # we initialize the list for storing states, actions, and rewards\n",
    "    states, actions, rewards = [], [], []\n",
    "    \n",
    "    # Initialize the gym environment\n",
    "    observation = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # append the states to the states list\n",
    "        states.append(observation)\n",
    "        \n",
    "        # now, we select an action using our sample_policy function and append the action to actions list\n",
    "         \n",
    "        action = sample_policy(observation)\n",
    "        actions.append(action)\n",
    "        \n",
    "        # We perform the action in the environment according to our sample_policy, move to the next state \n",
    "        # and receive reward\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # Break if the state is a terminal state\n",
    "        if (terminated or truncated ):\n",
    "             break\n",
    "                \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that we learned how to generate an episode, we will see how to perform First Vist MC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def every_visit_mc_prediction(policy, env, n_episodes):\n",
    "    \n",
    "    # First, we initialize the empty value table as a dictionary for storing the values of each state\n",
    "    value_table = defaultdict(float)\n",
    "    N = defaultdict(int)\n",
    "\n",
    "    returns = 0\n",
    "    for _ in range(n_episodes):\n",
    "        \n",
    "        # Next, we generate the epsiode and store the states and rewards\n",
    "        states, _, rewards = generate_episode(policy, env)\n",
    "        \n",
    "        \n",
    "        # Then for each step, we store the rewards to a variable R and states to S, and we calculate\n",
    "        # returns as a sum of rewards\n",
    "        \n",
    "        for t in range(len(states) - 1, -1, -1):\n",
    "            R = rewards[t]\n",
    "            S = states[t]\n",
    "            \n",
    "            returns += R\n",
    "            \n",
    "            # Now to perform first visit MC, we check if the episode is visited for the first time, if yes,\n",
    "            # we simply take the average of returns and assign the value of the state as an average of returns\n",
    "            \n",
    "            if S in states[:t]:\n",
    "                N[S] += 1\n",
    "                value_table[S] += (returns - value_table[S]) / N[S]\n",
    "    \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/453543/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "value = every_visit_mc_prediction(sample_policy, env, n_episodes=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see first few elements in the value table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'popitem(): dictionary is empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m   \u001b[38;5;28mprint\u001b[39m(value\u001b[38;5;241m.\u001b[39mpopitem())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'popitem(): dictionary is empty'"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(value.popitem())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We define the function plot_blackjack for plotting the value function and we can see how our value function is attaining the convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_blackjack(V, ax1, ax2):\n",
    "    player_sum = np.arange(12, 21 + 1)\n",
    "    dealer_show = np.arange(1, 10 + 1)\n",
    "    usable_ace = np.array([False, True])\n",
    "    state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace)))\n",
    "\n",
    "    for i, player in enumerate(player_sum):\n",
    "        for j, dealer in enumerate(dealer_show):\n",
    "            for k, ace in enumerate(usable_ace):\n",
    "                state_values[i, j, k] = V[player, dealer, ace]\n",
    "    \n",
    "    X, Y = np.meshgrid(player_sum, dealer_show)\n",
    " \n",
    "    ax1.plot_wireframe(X, Y, state_values[:, :, 0])\n",
    "    ax2.plot_wireframe(X, Y, state_values[:, :, 1])\n",
    " \n",
    "    for ax in ax1, ax2:\n",
    "        ax.set_zlim(-1, 1)\n",
    "        ax.set_ylabel('player sum')\n",
    "        ax.set_xlabel('dealer showing')\n",
    "        ax.set_zlabel('state-value')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = pyplot.subplots(nrows=2, figsize=(5, 8),\n",
    "subplot_kw={'projection': '3d'})\n",
    "axes[0].set_title('value function without usable ace')\n",
    "axes[1].set_title('value function with usable ace')\n",
    "plot_blackjack(value, axes[0], axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m                 V[state_idx] \u001b[38;5;241m=\u001b[39m returns_sum[state_idx] \u001b[38;5;241m/\u001b[39m returns_count[state_idx]\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m V\n\u001b[0;32m---> 40\u001b[0m V \u001b[38;5;241m=\u001b[39m mc_prediction(env, \u001b[38;5;241m500000\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(V)\n",
      "Cell \u001b[0;32mIn[37], line 29\u001b[0m, in \u001b[0;36mmc_prediction\u001b[0;34m(env, num_episodes)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(states):\n\u001b[1;32m     28\u001b[0m     sum_val, dealer_card \u001b[38;5;241m=\u001b[39m state\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sum_val[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m31\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m dealer_card[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m     30\u001b[0m         state_idx \u001b[38;5;241m=\u001b[39m (sum_val[\u001b[38;5;241m0\u001b[39m], dealer_card[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Assuming usable_ace is always 0\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m state_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m returns_sum:\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "def generate_episode(env):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()  # Random policy\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if (terminated or truncated):\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def mc_prediction(env, num_episodes):\n",
    "    returns_sum = {}\n",
    "    returns_count = {}\n",
    "    V = np.zeros((32, 11, 2))\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode(env)\n",
    "        states, _, _ = zip(*episode)\n",
    "        discounts = np.array([0.9**i for i in range(len(episode)+1)])\n",
    "        for i, state in enumerate(states):\n",
    "            sum_val, dealer_card = state\n",
    "            if 0 <= sum_val[0] <= 31 and 1 <= dealer_card[0] <= 10:\n",
    "                state_idx = (sum_val[0], dealer_card[0], 0)  # Assuming usable_ace is always 0\n",
    "                if state_idx not in returns_sum:\n",
    "                    returns_sum[state_idx] = 0\n",
    "                    returns_count[state_idx] = 0\n",
    "                returns_sum[state_idx] += sum([r * discounts[j] for j, (_, _, r) in enumerate(episode[i:])])\n",
    "                returns_count[state_idx] += 1\n",
    "                V[state_idx] = returns_sum[state_idx] / returns_count[state_idx]\n",
    "\n",
    "    return V\n",
    "\n",
    "V = mc_prediction(env, 500000)\n",
    "print(V)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
