{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing BlackJack with epsilon-greedy policy MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First, let us import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Simulate the Blackjack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1', render_mode='human')\n",
    "#env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dictionary for storing the Q values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dictionary for storing the total return of the state-action pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dictionary for storing the count of the number of times a state-action pair is visited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dictionary for storing the count of the number of times a state-action pair is visited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q):\n",
    "    epsilon = 0.5\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)), key = lambda x: Q[(state,x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's generate an episode using the epsilon-greedy policy. We define a function called generate_episode, which takes the Q value as an input and returns the episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set the number of time steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(Q):\n",
    "    episode =[]\n",
    "    state = env.reset()[0]\n",
    "    for t in range(num_timesteps):\n",
    "        action = epsilon_greedy_policy(state, Q)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode.append((state,action, reward))\n",
    "        if (terminated or truncated):\n",
    "            break\n",
    "        state = next_state\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's learn how to compute the optimal policy. First, let's set the number of iterations, that is, the number of episodes, we want to generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/453543/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 500000\n",
    "for i in range(num_iterations):\n",
    "    episode = generate_episode(Q)\n",
    "    # get all state action pair in the episode\n",
    "    all_state_action_pairs = [(s,a) for (s,a,r) in episode]\n",
    "    # Store all the rewards obtained in the episode in the rewards list:\n",
    "    rewards = [r for (s, a, r) in episode]\n",
    "    for t, (state, action,_) in enumerate(episode):\n",
    "        if not (state, action) in all_state_action_pairs[0:t]:\n",
    "            R = sum(rewards[t:])\n",
    "            total_return[(state,action)] = total_return[(state,action)] + R\n",
    "            N[(state, action)] += 1\n",
    "            Q[(state,action)] = total_return[(state, action)] /N[(state, action)]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_action pair</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((13, 9, False), 1)</td>\n",
       "      <td>-0.488686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((9, 9, False), 1)</td>\n",
       "      <td>-0.223282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((13, 9, False), 0)</td>\n",
       "      <td>-0.584177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((17, 6, False), 0)</td>\n",
       "      <td>-0.024023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((17, 6, False), 1)</td>\n",
       "      <td>-0.585973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((17, 1, False), 1)</td>\n",
       "      <td>-0.760643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((18, 1, False), 0)</td>\n",
       "      <td>-0.357638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((14, 10, False), 0)</td>\n",
       "      <td>-0.568738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((14, 10, False), 1)</td>\n",
       "      <td>-0.610974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((6, 10, False), 1)</td>\n",
       "      <td>-0.472445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((7, 9, False), 0)</td>\n",
       "      <td>-0.569231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       state_action pair     value\n",
       "0    ((13, 9, False), 1) -0.488686\n",
       "1     ((9, 9, False), 1) -0.223282\n",
       "2    ((13, 9, False), 0) -0.584177\n",
       "3    ((17, 6, False), 0) -0.024023\n",
       "4    ((17, 6, False), 1) -0.585973\n",
       "5    ((17, 1, False), 1) -0.760643\n",
       "6    ((18, 1, False), 0) -0.357638\n",
       "7   ((14, 10, False), 0) -0.568738\n",
       "8   ((14, 10, False), 1) -0.610974\n",
       "9    ((6, 10, False), 1) -0.472445\n",
       "10    ((7, 9, False), 0) -0.569231"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(Q.items(),columns=['state_action pair','value'])\n",
    "df.head(11)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
